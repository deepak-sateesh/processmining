{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the imports here\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualization\n",
    "from pm4py.objects.conversion.dfg import converter as dfg_mining\n",
    "from pm4py.visualization.petrinet import factory as pn_vis_factory\n",
    "from collections import defaultdict \n",
    "import pandas as pd\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.adapters.pandas import csv_import_adapter\n",
    "from pm4py.objects.conversion.log import factory as conversion_factory\n",
    "from pm4py.util import constants\n",
    "import math\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from pm4py.objects.log.util import sorting\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####   All auxilary functions   ########\n",
    "#Checks for the largest common prefix  \n",
    "def lcp(s, t):  \n",
    "  n = min(len(s),len(t));  \n",
    "  for i in range(0,n):  \n",
    "    if(s[i] != t[i]):  \n",
    "      return s[0:i];  \n",
    "  else:  \n",
    "    return s[0:n];  \n",
    "\n",
    "def Find_sequence(eventList):\n",
    "    lrs=\"\";  \n",
    "    n = len(eventList);  \n",
    "    for i in range(0,n):  \n",
    "      for j in range(i+1,n):  \n",
    "        #Checks for the largest common factors in every substring  \n",
    "        x = lcp(eventList[i:n],eventList[j:n]);  \n",
    "            #If the current prefix is greater than previous one   \n",
    "            #then it takes the current one as longest repeating sequence  \n",
    "        if(len(x) > len(lrs)):\n",
    "            lrs=x;\n",
    "    \n",
    "          \n",
    "            \n",
    "    if(len(set(lrs))>1):\n",
    "        return (lrs);  \n",
    "\n",
    "def find_duplicate_events(x): \n",
    "    _size = len(x) \n",
    "    duplicate_list = [] \n",
    "    for i in range(_size): \n",
    "        k = i + 1\n",
    "        for j in range(k, _size): \n",
    "            if x[i] == x[j] and x[i] not in duplicate_list: \n",
    "                duplicate_list.append(x[i]) \n",
    "    return duplicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetectWeaknesses(logSorted, blacklist, maxTime, df, cols):\n",
    "    \n",
    "    #print(\"In detectWeaknesses function\",df)\n",
    "    for case_index, case in enumerate(logSorted):\n",
    "        eventList=[]\n",
    "        lrs=\"\"\n",
    "        indexList=[]\n",
    "        d={}\n",
    "        l=\"\"\n",
    "        prev=\"\"\n",
    "        d2={}\n",
    "        l2=[]\n",
    "        prev_end_timestamp=0\n",
    "        idle_time=0\n",
    "        prev_activity=\"\"\n",
    "        duration=0 \n",
    "        a=\"\"\n",
    "        max_duration=0\n",
    "        for event_index, event in enumerate(case):\n",
    "            eventList.append(event[\"Activity\"])  \n",
    "            if(event[\"Activity\"] in blacklist):\n",
    "                #print(\"Unwanted activity=> activity: %s -> case: %s that started @ %s \" % (event[\"Activity\"], event[\"Case ID\"], event[\"Start Timestamp\"]))\n",
    "                row={cols[0]:event[\"Case ID\"]+\"-> Event \"+str(event_index), cols[1]: event[\"Case ID\"], cols[2]:'AF', cols[3]:'1',cols[4]:'Expert',cols[5]:event[\"Start Timestamp\"],cols[6]:'Unwanted activity \\\"'+event[\"Activity\"]+'\\\"',cols[7]:'In the case', cols[8]:'Event level'}\n",
    "                df=df.append(row, ignore_index=True)\n",
    "                \n",
    "            if( prev!=\"\" and  event[\"Resource\"]!= prev):\n",
    "                #print(\"The resource has changed for the activity: \\\"%s\\\" from \\\"%s\\\" to \\\"%s\\\"\"%(event[\"Activity\"], prev, event[\"Resource\"]))\n",
    "                row={cols[0]:event[\"Case ID\"]+\"-> Event \"+str(event_index), cols[1]: event[\"Case ID\"], cols[2]:'AF', cols[3]:'4',cols[4]:'Automatic detection',cols[5]:event[\"Start Timestamp\"],cols[6]:'Change of interface for activity '+event[\"Activity\"]+' from ' +prev+' to '+ event[\"Resource\"],cols[7]:'In the case',cols[8]:'Event Level'}\n",
    "                df=df.append(row, ignore_index=True)\n",
    "            prev=event[\"Resource\"]\n",
    "            \n",
    "            if(prev_end_timestamp!=0):\n",
    "                idle_time=pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-prev_end_timestamp\n",
    "            if(type(idle_time)!= int and idle_time.total_seconds()>maxTime):\n",
    "                #.total_seconds()>7200) :\n",
    "                row={cols[0]:event[\"Case ID\"]+\"-> Event \"+str(event_index), cols[1]: event[\"Case ID\"], cols[2]:'PA', cols[3]:'6',cols[4]:'Expert',cols[5]:event[\"Start Timestamp\"],cols[6]:'Idletime between '+prev_activity+' to ' +event[\"Activity\"]+' is '+ str(idle_time),cols[7]:'In the case',cols[8]:'Event level'}\n",
    "                df=df.append(row, ignore_index=True)\n",
    "            prev_end_timestamp=pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\") \n",
    "            prev_activity=event[\"Activity\"]\n",
    "            \n",
    "            if event[\"Activity\"] not in d2.keys():\n",
    "                l2=[]\n",
    "            else:\n",
    "                l2=d2[event[\"Activity\"]]\n",
    "            l2.append( (pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\"))/ pd.Timedelta(hours=1))\n",
    "            d2[event[\"Activity\"]]=(l2)\n",
    "            \n",
    "            duration=pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")\n",
    "            if(max_duration==0 or duration>max_duration):\n",
    "                max_duration=duration\n",
    "                a=event[\"Activity\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "                \n",
    "        if(Find_sequence(eventList) is not None ):\n",
    "            lrs=Find_sequence(eventList)\n",
    "            #print(\"Repeating sequence for events in case:\",case.attributes['concept:name'],\" is: \", lrs)  \n",
    "            row={cols[0]:case.attributes['concept:name'], cols[1]: case.attributes['concept:name'], cols[2]:'AF', cols[3]:'2',cols[4]:'Automatic detection',cols[5]:'',cols[6]:'Backloop {'+''.join(lrs)+'}',cols[7]:'In the case',cols[8]:'Case level'}\n",
    "            df=df.append(row, ignore_index=True)\n",
    "            \n",
    "        duplicateEventList=[]\n",
    "        duplicateEventList=find_duplicate_events(eventList)\n",
    "        #print (\"The events which got repeated in the trace are\",duplicateEventList)\n",
    "        if(len(duplicateEventList)>0):\n",
    "            row={cols[0]:case.attributes[\"concept:name\"], cols[1]: event[\"Case ID\"], cols[2]:'AF', cols[3]:'3',cols[4]:'Automatic detection',cols[5]:\"\",cols[6]:'Redundant Activities list: \\\"'+''.join(duplicateEventList)+'\\\"',cols[7]:'In the case', cols[8]:'Case Level'}\n",
    "            df=df.append(row, ignore_index=True)\n",
    "            \n",
    "    \n",
    "    row={cols[0]:\"All Activities\", cols[1]: a, cols[2]:'PA', cols[3]:'8',cols[4]:'Automatic detection',cols[5]:'',cols[6]:'Activity took maximum time of '+str(max_duration),cols[7]:'In the Activity',cols[8]:'Log Level'}\n",
    "    df=df.append(row, ignore_index=True)\n",
    "    \n",
    "    variance_dict={}\n",
    "    for k,v in d2.items():\n",
    "        variance_dict[k]= (min(v), max(v),np.mean(v) ,np.var(v))\n",
    "        row={cols[0]:\"All Activities\", cols[1]: k, cols[2]:'PA', cols[3]:'7',cols[4]:'Automatic detection',cols[5]:'',cols[6]:'(Min, Max, Average, Variance) for current activity:'+str((min(v), max(v),np.mean(v) ,np.var(v))),cols[7]:'In the Activity',cols[8]:'Activity Level'}\n",
    "        df=df.append(row, ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallelizable_tasks_CaseLevel(df,cols):\n",
    "\n",
    "    print(\"Parallelizable_tasks_CaseLevel function\\n\\n\")\n",
    "    log_csv3 = pd.read_csv('Production_Data.csv', sep=',')\n",
    "    log_csv3.rename(columns={'Activity': 'concept:name'}, inplace=True)\n",
    "    parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'Case ID'}\n",
    "    event_log3 = log_converter.apply(log_csv3, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "    event_log3 = sorting.sort_timestamp(event_log3,\"Start Timestamp\", False)\n",
    "    \n",
    "    for case_index, case in enumerate(event_log3):\n",
    "        \n",
    "        tracefilter_log_pos = attributes_filter.apply(event_log3, [case.attributes[\"concept:name\"]],\n",
    "                                          parameters={attributes_filter.Parameters.ATTRIBUTE_KEY : \"Case ID\", attributes_filter.Parameters.POSITIVE: True})\n",
    "        \n",
    "        dfg_simple3 = dfg_discovery.apply(tracefilter_log_pos)\n",
    "        l=[]\n",
    "        for k in dfg_simple3.keys():\n",
    "            if(k[0]!=k[1]):\n",
    "                if (k[1],k[0]) in dfg_simple3.keys():\n",
    "                    l.append((k[1],k[0]))\n",
    "        l1=[]\n",
    "        for i in l:\n",
    "            if (i[1],i[0]) in l:\n",
    "                l1.append((i[0],i[1]))\n",
    "                l.remove((i[1],i[0]))\n",
    "                l.remove((i[0],i[1]))\n",
    "        if(len(l)>0)   :  \n",
    "            row={cols[0]:case.attributes['concept:name'], cols[1]: case.attributes['concept:name'], cols[2]:'AF', cols[3]:'9',cols[4]:'Automatic detection',cols[5]:'',cols[6]:'Parallelizable tasks :'+''.join(str(l1)),cols[7]:'In the case',cols[8]:'Case level'}\n",
    "            df=df.append(row, ignore_index=True)\n",
    "            #print(\"\\n\\nParallelizable tasks for Case:\",case.attributes[\"concept:name\"],\" are => \", end=\" \")\n",
    "            #print(l1)\n",
    "        \n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Joint Master thesis:\n",
      "Modelling of production expertise to extend the data-driven analysis of process models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepak/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecatedWarning: apply is deprecated as of 1.3.0 and will be removed in 2.0.0. Use algorithm entrypoint instead\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallelizable_tasks_CaseLevel function\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main(): \n",
    "    print(\"Welcome to Joint Master thesis:\\nModelling of production expertise to extend the data-driven analysis of process models\") \n",
    "    ######All initialisations############\n",
    "    blacklist=[ 'Lapping - Machine 1','Turning & Milling - Machine 8']\n",
    "    cols=['Detected Weakness Row','Case ID','Weakness Type (AF/PA)','Weakness ID','Weakness Origin', 'Weakness Time','Weakness Information','Weakness Measurement', 'Weakness Level']\n",
    "    df=pd.DataFrame(columns=cols)\n",
    "    \n",
    "    \n",
    "    #log = xes_importer.apply('running-example.xes')\n",
    "    log_csv = pd.read_csv('Production_Data.csv', sep=',')\n",
    "    log = conversion_factory.apply(log_csv, parameters={constants.PARAMETER_CONSTANT_CASEID_KEY: \"Case ID\",\n",
    "                                                   constants.PARAMETER_CONSTANT_ACTIVITY_KEY: \"Activity\",\n",
    "                                                    constants.PARAMETER_CONSTANT_START_TIMESTAMP_KEY:\"Start Timestamp\",\n",
    "                                                    constants.PARAMETER_CONSTANT_RESOURCE_KEY:\"Resource\",\n",
    "                                                    constants.PARAMETER_CONSTANT_TIMESTAMP_KEY:\"Complete Timestamp\"\n",
    "                                                   })\n",
    "\n",
    "    \n",
    "    maxTime=86400\n",
    "    logSorted = sorting.sort_timestamp(log,\"Start Timestamp\", False)\n",
    "    df=DetectWeaknesses(logSorted, blacklist, maxTime,df,cols)\n",
    "    df=Parallelizable_tasks_CaseLevel(df,cols)\n",
    "    df.to_excel(\"Integrated_dataframe.xlsx\")\n",
    "    #print(df)\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ltl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-4f6550c8196b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mltl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mltl_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr_value_different_persons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ltl' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
