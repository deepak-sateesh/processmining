{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the imports here\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualization\n",
    "from pm4py.objects.conversion.dfg import converter as dfg_mining\n",
    "from pm4py.visualization.petrinet import factory as pn_vis_factory\n",
    "from collections import defaultdict \n",
    "import pandas as pd\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.adapters.pandas import csv_import_adapter\n",
    "from pm4py.objects.conversion.log import factory as conversion_factory\n",
    "from pm4py.util import constants\n",
    "import math\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from pm4py.objects.log.util import sorting\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_log(log):\n",
    "    for case_index, case in enumerate(log):\n",
    "        print(\"\\n case index: %d  case id: %s\" % (case_index, case.attributes[\"concept:name\"]))\n",
    "        for event_index, event in enumerate(case):\n",
    "            print(\"event index: %d  event activity: %s\" % (event_index, event[\"concept:name\"]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class represents a directed graph using dfg \n",
    "class Graph: \n",
    "\n",
    "    def __init__(self,vertices): \n",
    "        self.V= vertices #No. of vertices \n",
    "        self.graph = defaultdict(list) # default dictionary to store graph \n",
    "\n",
    "    # function to add an edge to graph \n",
    "    def addEdge(self,u,v): \n",
    "        self.graph[u].append(v) \n",
    "    \n",
    "    # Use BFS to check path between s and d \n",
    "    def isReachable(self, s, d): \n",
    "        # Mark all the vertices as not visited \n",
    "        visited =[False]*(self.V) \n",
    "\n",
    "        # Create a queue for BFS \n",
    "        queue=[]\n",
    "\n",
    "        # Mark the source node as visited and enqueue it \n",
    "        queue.append(s) \n",
    "        visited[s] = True\n",
    "\n",
    "        while queue: \n",
    "\n",
    "            #Dequeue a vertex from queue \n",
    "            n = queue.pop(0) \n",
    "\n",
    "            # If this adjacent node is the destination node, \n",
    "            # then return true \n",
    "            if n == d: \n",
    "                return True\n",
    "\n",
    "            # Else, continue to do BFS \n",
    "            for i in self.graph[n]: \n",
    "                if visited[i] == False: \n",
    "                    queue.append(i) \n",
    "                    visited[i] = True\n",
    "        # If BFS is complete without visited d \n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weakness(log, forbidden_sequence):\n",
    "    #Weakness 1: Duplicate or loop-> Same event repeating twice in the log\n",
    "    for case_index, case in enumerate(log):\n",
    "        print(\"\\n case index: %d  case id: %s\" % (case_index, case.attributes[\"concept:name\"]))\n",
    "        event_list=[]\n",
    "        \n",
    "        for event_index, event in enumerate(case):\n",
    "            print(\"event index: %d  event activity: %s\" % (event_index, event[\"concept:name\"]))\n",
    "            event_list.append(event[\"concept:name\"])\n",
    "            \n",
    "        print (\"The events which got repeated in the trace are\",find_duplicate_events(event_list))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    #Weakness 2: Find out if the forbidden sequence of events exists in the log\n",
    "    #applying the Directly follows graph discovery to get the sequence which are directly following each other\n",
    "    dfg_simple = dfg_discovery.apply(log)\n",
    "    violated_restrictions=[]#for directly following each other or indirectly following\n",
    "    for r in forbidden_sequence:\n",
    "        count=0\n",
    "        for d in dfg_simple.elements():\n",
    "            if(r==d):\n",
    "                count+=1\n",
    "                violated_restrictions.append((r,count))\n",
    "            #else if(r[0]==d[0]):\n",
    "            \n",
    "              \n",
    "                \n",
    "    print(\"Violated restrictions, Number of times violated: \",violated_restrictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d683a05d86a24d64b12a1b4fe0e52b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='parsing log, completed traces :: ', max=6.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There is a path from register request to decide\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a graph for the given dfg\n",
    "\n",
    "log = xes_importer.apply('running-example.xes')\n",
    "dfg_simple = dfg_discovery.apply(log)\n",
    "\n",
    "    \n",
    "g = Graph(len(list(dfg_simple.elements())))\n",
    "l=[]\n",
    "for t in dfg_simple.elements(): \n",
    "    for x in t: \n",
    "        l.append(x) \n",
    "l=list(set(l))#list mapping every element to a number\n",
    "for d in dfg_simple.elements():\n",
    "    g.addEdge(l.index(d[0]),l.index(d[1]))\n",
    "    \n",
    "\n",
    "u =l.index(\"register request\"); v = l.index(\"decide\")\n",
    "\n",
    "if g.isReachable(u, v): \n",
    "    print(\"There is a path from %s to %s\" % (l[u],l[v])) \n",
    "else : \n",
    "    print(\"There is no path from %s to %s\" % (l[u],l[v])) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c91337d46e74926989a41c6723d9626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='parsing log, completed traces :: ', max=6.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.EventLog'>\n"
     ]
    }
   ],
   "source": [
    "log = xes_importer.apply('running-example.xes')\n",
    "dfg_simple = dfg_discovery.apply(log)\n",
    "\n",
    "for case_index, case in enumerate(log):\n",
    "    print(type(case))\n",
    "    #dfg_simple1 = dfg_discovery.apply(case)\n",
    "    \n",
    "\n",
    "print(type(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.objects.dfg.utils import dfg_utils\n",
    "from pm4py.objects.petri.petrinet import PetriNet, Marking\n",
    "from pm4py.objects.petri import utils as pn_util\n",
    "from enum import Enum\n",
    "from pm4py.util import exec_utils\n",
    "\n",
    "\n",
    "class Parameters(Enum):\n",
    "    START_ACTIVITIES = 'start_activities'\n",
    "    END_ACTIVITIES = 'end_activities'\n",
    "\n",
    "\n",
    "\n",
    "PARAM_KEY_START_ACTIVITIES = Parameters.START_ACTIVITIES\n",
    "PARAM_KEY_END_ACTIVITIES = Parameters.END_ACTIVITIES\n",
    "\n",
    "#obtain petrinet from dfg\n",
    "def obtain_petrinet_from_dfg(dfg, parameters=None):\n",
    "    \"\"\"\n",
    "    Applies the DFG mining on a given object (if it is a Pandas dataframe or a log, the DFG is calculated)\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    dfg\n",
    "        Object (DFG) (if it is a Pandas dataframe or a log, the DFG is calculated)\n",
    "    parameters\n",
    "        Parameters\n",
    "    \"\"\"\n",
    "    if parameters is None:\n",
    "        parameters = {}\n",
    "\n",
    "    dfg = dfg\n",
    "    start_activities = exec_utils.get_param_value(Parameters.START_ACTIVITIES, parameters,\n",
    "                                                  dfg_utils.infer_start_activities(\n",
    "                                                      dfg))\n",
    "    end_activities = exec_utils.get_param_value(Parameters.END_ACTIVITIES, parameters,\n",
    "                                                dfg_utils.infer_end_activities(dfg))\n",
    "    activities = dfg_utils.get_activities_from_dfg(dfg)\n",
    "\n",
    "    net = PetriNet(\"\")\n",
    "    im = Marking()\n",
    "    fm = Marking()\n",
    "\n",
    "    source = PetriNet.Place(\"source\")\n",
    "    net.places.add(source)\n",
    "    im[source] = 1\n",
    "    sink = PetriNet.Place(\"sink\")\n",
    "    net.places.add(sink)\n",
    "    fm[sink] = 1\n",
    "\n",
    "    places_corr = {}\n",
    "    index = 0\n",
    "\n",
    "    for act in activities:\n",
    "        places_corr[act] = PetriNet.Place(act)\n",
    "        net.places.add(places_corr[act])\n",
    "\n",
    "    for act in start_activities:\n",
    "        if act in places_corr:\n",
    "            index = index + 1\n",
    "            trans = PetriNet.Transition(act + \"_\" + str(index), act)\n",
    "            net.transitions.add(trans)\n",
    "            pn_util.add_arc_from_to(source, trans, net)\n",
    "            pn_util.add_arc_from_to(trans, places_corr[act], net)\n",
    "\n",
    "    for act in end_activities:\n",
    "        if act in places_corr:\n",
    "            index = index + 1\n",
    "            inv_trans = PetriNet.Transition(act + \"_\" + str(index), None)\n",
    "            net.transitions.add(inv_trans)\n",
    "            pn_util.add_arc_from_to(places_corr[act], inv_trans, net)\n",
    "            pn_util.add_arc_from_to(inv_trans, sink, net)\n",
    "\n",
    "    for el in dfg.keys():\n",
    "        act1 = el[0]\n",
    "        act2 = el[1]\n",
    "\n",
    "        index = index + 1\n",
    "        trans = PetriNet.Transition(act2 + \"_\" + str(index), act2)\n",
    "        net.transitions.add(trans)\n",
    "\n",
    "        pn_util.add_arc_from_to(places_corr[act1], trans, net)\n",
    "        pn_util.add_arc_from_to(trans, places_corr[act2], net)\n",
    "\n",
    "    return net, im, fm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'row={cols[0]:1, cols[1]: \\'Case 10\\', cols[2]:\\'FA\\',cols[4]:\\'User\\',cols[5]:\\'10/02/2020 12:20\\',cols[6]:\\'Unwanted activity “a\"\\',cols[7]:\\'Happened in the case\\'}\\n#cols[3]:\\'1\\'\\ndf = df.append(row, ignore_index=True)'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''cols=['Detected Weakness Row','Case ID','Weakness Type (AF/PA)','Weakness ID','Weakness Origin', 'Weakness Time','Weakness Information','Weakness Measurement']\n",
    "df=pd.DataFrame(columns=cols)'''\n",
    "#len(cols)\n",
    "\n",
    "'''row={cols[0]:1, cols[1]: 'Case 10', cols[2]:'FA',cols[4]:'User',cols[5]:'10/02/2020 12:20',cols[6]:'Unwanted activity “a\"',cols[7]:'Happened in the case'}\n",
    "#cols[3]:'1'\n",
    "df = df.append(row, ignore_index=True)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unwanted_Activity(log, blacklist):\n",
    "    global df\n",
    "    print(\"Unwanted activity function\")\n",
    "    for case_index, case in enumerate(log):\n",
    "        for event_index, event in enumerate(case):\n",
    "            if(event[\"Activity\"] in blacklist):\n",
    "                print(\"Unwanted activity=> activity: %s -> case: %s that started @ %s \" % (event[\"Activity\"], event[\"Case ID\"], event[\"Start Timestamp\"]))\n",
    "                row={cols[0]:event[\"Case ID\"]+\"->\"+str(event_index), cols[1]: event[\"Case ID\"], cols[2]:'AF',cols[4]:'Expert',cols[5]:event[\"Start Timestamp\"],cols[6]:'Unwanted activity \\\"event[\"Activity\"]\\\"',cols[7]:'In the case'}\n",
    "                df = df.append(row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks for the largest common prefix  \n",
    "def lcp(s, t):  \n",
    "  n = min(len(s),len(t));  \n",
    "  for i in range(0,n):  \n",
    "    if(s[i] != t[i]):  \n",
    "      return s[0:i];  \n",
    "  else:  \n",
    "    return s[0:n];  \n",
    "\n",
    "def Find_sequence(eventList):\n",
    "    lrs=\"\";  \n",
    "    n = len(eventList);  \n",
    "    for i in range(0,n):  \n",
    "      for j in range(i+1,n):  \n",
    "        #Checks for the largest common factors in every substring  \n",
    "        x = lcp(eventList[i:n],eventList[j:n]);  \n",
    "            #If the current prefix is greater than previous one   \n",
    "            #then it takes the current one as longest repeating sequence  \n",
    "        if(len(x) > len(lrs)):  \n",
    "          lrs=x;    \n",
    "    if(len(set(lrs))>1):\n",
    "        return lrs;  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backloop(log):\n",
    "    print(\"Backloop function\")\n",
    "    \n",
    "    for case_index, case in enumerate(log):\n",
    "        eventList=[]\n",
    "        lrs=\"\"\n",
    "        for event_index, event in enumerate(case):\n",
    "            eventList.append(event[\"Activity\"])  \n",
    "        lrs=Find_sequence(eventList)\n",
    "        if(lrs is not None and lrs!=\"\"):\n",
    "            print(\"Repeating sequence for events in case:\",case.attributes['concept:name'],\" is: \", lrs)  \n",
    "        \n",
    "    #for trace in event_log:\n",
    "    #    print(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_events(x): \n",
    "    _size = len(x) \n",
    "    duplicate_list = [] \n",
    "    for i in range(_size): \n",
    "        k = i + 1\n",
    "        for j in range(k, _size): \n",
    "            if x[i] == x[j] and x[i] not in duplicate_list: \n",
    "                duplicate_list.append(x[i]) \n",
    "    return duplicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Redundant_Activity(log):\n",
    "    print(\"Redundant_Activity function\")\n",
    "    for case_index, case in enumerate(log):\n",
    "        print(\"\\n Case Id: %s\" % ( case.attributes[\"concept:name\"]))\n",
    "        event_list=[]\n",
    "        \n",
    "        for event_index, event in enumerate(case):\n",
    "            print(\"event start time: %s  event activity: %s\" % (event[\"Start Timestamp\"], event[\"Activity\"]))\n",
    "            event_list.append(event[\"Activity\"])\n",
    "            \n",
    "        print (\"The events which got repeated in the trace are\",find_duplicate_events(event_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interface(log):\n",
    "    print(\"Interface function\")\n",
    "    for case_index, case in enumerate(log):\n",
    "        d={}\n",
    "        l=\"\"\n",
    "        print(\"\\n Case Id: %s\" % ( case.attributes[\"concept:name\"]))\n",
    "        \n",
    "        '''for event_index, event in enumerate(case):\n",
    "            if( len(d)!=0 and event[\"Activity\"] in d.keys() and event[\"Resource\"]!= d[event[\"Activity\"]]):\n",
    "                print(\"The resource has changed for the activity: %s from %s to %s\"%(event[\"Activity\"], d[event[\"Activity\"]], event[\"Resource\"]))\n",
    "            d[event[\"Activity\"]]=event[\"Resource\"]'''\n",
    "        prev=\"\"\n",
    "        for event_index, event in enumerate(case):\n",
    "            if( prev!=\"\" and  event[\"Resource\"]!= prev):\n",
    "                print(\"The resource has changed for the activity: \\\"%s\\\" from \\\"%s\\\" to \\\"%s\\\"\"%(event[\"Activity\"], prev, event[\"Resource\"]))\n",
    "            prev=event[\"Resource\"]\n",
    "            \n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Switch_of_media(log):\n",
    "    print(\"Switch_of_media function\")\n",
    "    print(\"Logic is same as Interface function as there is no column for media in the given CSV \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Idle_time(log):\n",
    "    print(\"Idle_time function\")\n",
    "    for case_index, case in enumerate(log):\n",
    "        print(\"\\n Case Id: %s\" % ( case.attributes[\"concept:name\"]))\n",
    "        prev_end_timestamp=0\n",
    "        idle_time=0\n",
    "        prev_activity=\"\"\n",
    "        for event_index, event in enumerate(case):\n",
    "            if(prev_end_timestamp!=0):\n",
    "                idle_time=pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-prev_end_timestamp\n",
    "            print(\"Idle time between previous activity:%s and current activity:%s is %s\"%(prev_activity, event[\"Activity\"], idle_time))\n",
    "            prev_end_timestamp=pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\") \n",
    "            prev_activity=event[\"Activity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean1(log):\n",
    "    total_events=0\n",
    "    avg_dict={}\n",
    "    for case_index, case in enumerate(log): \n",
    "        for event_index, event in enumerate(case):\n",
    "            total_events=+1\n",
    "            if event[\"Activity\"] not in avg_dict.keys():\n",
    "                avg_dict[event[\"Activity\"]]=((pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\"))/ pd.Timedelta(hours=1),1)\n",
    "            else:\n",
    "                avg_dict[event[\"Activity\"]]=(avg_dict[event[\"Activity\"]][0] + (pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\"))/ pd.Timedelta(hours=1),(avg_dict[event[\"Activity\"]][1])+1)\n",
    "    \n",
    "    avg_dict2={}\n",
    "    for k, v in avg_dict.items():\n",
    "        avg_dict2[k]=v[0]/v[1]\n",
    "    print(avg_dict2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Variance_of_process_times(log):\n",
    "    print(\"Variance_of_process_times function\")\n",
    "    d={}#mean1(log)\n",
    "    l=[]\n",
    "    for case_index, case in enumerate(log): \n",
    "        for event_index, event in enumerate(case):\n",
    "            if event[\"Activity\"] not in d.keys():\n",
    "                l=[]\n",
    "            else:\n",
    "                l=d[event[\"Activity\"]]\n",
    "            l.append( (pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\"))/ pd.Timedelta(hours=1))\n",
    "            d[event[\"Activity\"]]=(l)\n",
    "    variance_dict={}\n",
    "    for k,v in d.items():\n",
    "        variance_dict[k]= (min(v), max(v),np.mean(v) ,np.var(v))\n",
    "    print(\"(Min, Max, Average, Variance) for each activity:\")\n",
    "    print(variance_dict)\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bottleneck(log):\n",
    "    print(\"Bottleneck function\")\n",
    "    '''for case_index, case in enumerate(log):\n",
    "        print(\"\\n Case Id: %s\" % ( case.attributes[\"concept:name\"]))\n",
    "        duration=0 \n",
    "        a=\"\"\n",
    "        max_duration=0\n",
    "        for event_index, event in enumerate(case):\n",
    "            duration=pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")\n",
    "            if(max_duration==0 or duration>max_duration):\n",
    "                max_duration=duration\n",
    "                a=event[\"Activity\"]\n",
    "        print(\"Bottleneck Activity at case level:%s took maximum time of %s to complete\"%(a,max_duration ))'''\n",
    "    duration=0 \n",
    "    a=\"\"\n",
    "    max_duration=0\n",
    "    for case_index, case in enumerate(log):\n",
    "        for event_index, event in enumerate(case):\n",
    "            duration=pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")\n",
    "            if(max_duration==0 or duration>max_duration):\n",
    "                max_duration=duration\n",
    "                a=event[\"Activity\"]\n",
    "    \n",
    "    print(\"Bottleneck Activity on log level:%s took maximum time of %s to complete\"%(a,max_duration ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallelizable_tasks_loglevel():\n",
    "    print(\"Parallelizable_tasks function\\n\\n\")\n",
    "    log_csv2 = pd.read_csv('Production_Data.csv', sep=',')\n",
    "    log_csv2.rename(columns={'Activity': 'concept:name'}, inplace=True)\n",
    "    parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'Case ID'}\n",
    "    event_log2 = log_converter.apply(log_csv2, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "    #print(event_log)\n",
    "    dfg_simple2 = dfg_discovery.apply(event_log2)\n",
    "    #print(dfg_simple2)\n",
    "    #('Turning & Milling Q.C.', 'Turning & Milling - Machine 8'): 27,\n",
    "    for k in dfg_simple2.keys():\n",
    "        if(k[0]!=k[1]):\n",
    "            if (k[1],k[0]) in dfg_simple2.keys():\n",
    "                print(k,\" : are Parallelizable activities\")\n",
    "         \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallelizable_tasks_CaseLevel():\n",
    "\n",
    "    print(\"Parallelizable_tasks_CaseLevel function\\n\\n\")\n",
    "    log_csv3 = pd.read_csv('Production_Data.csv', sep=',')\n",
    "    log_csv3.rename(columns={'Activity': 'concept:name'}, inplace=True)\n",
    "    parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'Case ID'}\n",
    "    event_log3 = log_converter.apply(log_csv3, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "    event_log3 = sorting.sort_timestamp(event_log3,\"Start Timestamp\", False)\n",
    "    \n",
    "    for case_index, case in enumerate(event_log3):\n",
    "        \n",
    "        tracefilter_log_pos = attributes_filter.apply(event_log3, [case.attributes[\"concept:name\"]],\n",
    "                                          parameters={attributes_filter.Parameters.ATTRIBUTE_KEY : \"Case ID\", attributes_filter.Parameters.POSITIVE: True})\n",
    "        \n",
    "        dfg_simple3 = dfg_discovery.apply(tracefilter_log_pos)\n",
    "        l=[]\n",
    "        for k in dfg_simple3.keys():\n",
    "            if(k[0]!=k[1]):\n",
    "                if (k[1],k[0]) in dfg_simple3.keys():\n",
    "                    l.append((k[1],k[0]))\n",
    "        l1=[]\n",
    "        for i in l:\n",
    "            if (i[1],i[0]) in l:\n",
    "                l1.append((i[0],i[1]))\n",
    "                l.remove((i[1],i[0]))\n",
    "                l.remove((i[0],i[1]))\n",
    "        if(len(l)>0)   :     \n",
    "            print(\"\\n\\nParallelizable tasks for Case:\",case.attributes[\"concept:name\"],\" are => \", end=\" \")\n",
    "            print(l1)\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Joint Master thesis:\n",
      "Modelling of production expertise to extend the data-driven analysis of process models\n",
      "Log imported\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepak/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: DeprecatedWarning: apply is deprecated as of 1.3.0 and will be removed in 2.0.0. Use algorithm entrypoint instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unwanted activity function\n",
      "Unwanted activity=> activity: Lapping - Machine 1 -> case: Case 203 that started @ 1/10/2012 0:00:00 \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-b370376f7013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m##Parallelizable_tasks_loglevel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#Parallelizable_tasks_CaseLevel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-148-b370376f7013>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mblacklist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'Lapping - Machine 1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Turning & Milling - Machine 8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mUnwanted_Activity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogSorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblacklist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#Backloop(logSorted)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-1b5f97cd40b6>\u001b[0m in \u001b[0;36mUnwanted_Activity\u001b[0;34m(log, blacklist)\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unwanted activity=> activity: %s -> case: %s that started @ %s \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Activity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Case ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Start Timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Case ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"->\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Case ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'AF'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Expert'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Start Timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Unwanted activity \\\"event[\"Activity\"]\\\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'In the case'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Defining main function \n",
    "def main(): \n",
    "    print(\"Welcome to Joint Master thesis:\\nModelling of production expertise to extend the data-driven analysis of process models\") \n",
    "    \n",
    "    '''#Import a log\n",
    "    log = xes_importer.apply('running-example.xes')\n",
    "    print(\"Log imported\")\n",
    "    \n",
    "    #Explore the log\n",
    "    #explore_log(log)\n",
    "    \n",
    "    #Define the forbidden sequence of events\n",
    "    #simple restriction which says you cannot decide without examining thoroughly \n",
    "    forbidden_sequence=[( 'decide','examine thoroughly')]\n",
    "    \n",
    "    #Find different kinds of weakness in the log\n",
    "    find_weakness(log, forbidden_sequence)\n",
    "    \n",
    "    #obtain_petrinet_from_dfg\n",
    "    dfg_simple = dfg_discovery.apply(log)\n",
    "    net, im, fm = obtain_petrinet_from_dfg(dfg_simple)\n",
    "\n",
    "    #Visualise the petrinet obtained\n",
    "    gviz = pn_vis_factory.apply(net, im, fm)\n",
    "    pn_vis_factory.view(gviz)'''\n",
    "    \n",
    "    #log = xes_importer.apply('running-example.xes')\n",
    "    log_csv = pd.read_csv('Production_Data.csv', sep=',')\n",
    "    log = conversion_factory.apply(log_csv, parameters={constants.PARAMETER_CONSTANT_CASEID_KEY: \"Case ID\",\n",
    "                                                   constants.PARAMETER_CONSTANT_ACTIVITY_KEY: \"Activity\",\n",
    "                                                    constants.PARAMETER_CONSTANT_START_TIMESTAMP_KEY:\"Start Timestamp\",\n",
    "                                                    constants.PARAMETER_CONSTANT_RESOURCE_KEY:\"Resource\",\n",
    "                                                    constants.PARAMETER_CONSTANT_TIMESTAMP_KEY:\"Complete Timestamp\"\n",
    "                                                   })\n",
    "    print(\"Log imported\\n\\n\\n\")\n",
    "    \n",
    "    logSorted = sorting.sort_timestamp(log,\"Start Timestamp\", False)\n",
    "    cols=['Detected Weakness Row','Case ID','Weakness Type (AF/PA)','Weakness ID','Weakness Origin', 'Weakness Time','Weakness Information','Weakness Measurement']\n",
    "    df=pd.DataFrame(columns=cols)\n",
    "    #print(log)'sorted_log' x.attributes[\"concept:name\"], x.events[\"Start Timestamp\"]\n",
    "    #for i in log:\n",
    "    #    print(sorted(i),events['Start Timestamp'])\n",
    "    \n",
    "    blacklist=[ 'Lapping - Machine 1','Turning & Milling - Machine 8']\n",
    "    Unwanted_Activity(logSorted, blacklist)\n",
    "    \n",
    "    #Backloop(logSorted)\n",
    "    \n",
    "    #Redundant_Activity(logSorted) \n",
    "    \n",
    "    #Interface(logSorted)\n",
    "    \n",
    "    #Switch_of_media(logSorted) Same as Interface as there is no column for media in the given CSV \n",
    "    \n",
    "    #Idle_time(logSorted)\n",
    "    \n",
    "    #Variance_of_process_times(logSorted)\n",
    "    \n",
    "    #Bottleneck(logSorted)\n",
    "    \n",
    "    ##Parallelizable_tasks_loglevel()\n",
    "    #Parallelizable_tasks_CaseLevel()\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log imported\n",
      "\n",
      "\n",
      "\n",
      "###########################\n",
      " [{'attributes': {'concept:name': 'Case 1'}, 'events': [{'Case ID': 'Case 1', 'Activity': 'Turning & Milling - Machine 4', 'Resource': 'Machine 4 - Turning & Milling', 'Start Timestamp': '1/29/2012 23:24:00', 'Complete Timestamp': '1/30/2012 5:43:00', 'Span': '006:19', 'Work Order  Qty': 10, 'Part Desc.': 'Cable Head', 'Worker ID': 'ID4932', 'Report Type': 'S', 'Qty Completed': 1, 'Qty Rejected': 0, 'Qty for MRB': 0, 'Rework': nan, 'duration': '19:00.0'}, '..', {'Case ID': 'Case 1', 'Activity': 'Packing', 'Resource': 'Packing', 'Start Timestamp': '2/17/2012 0:00:00', 'Complete Timestamp': '2/17/2012 1:00:00', 'Span': '000:00', 'Work Order  Qty': 10, 'Part Desc.': 'Cable Head', 'Worker ID': 'ID4820', 'Report Type': 'D', 'Qty Completed': 9, 'Qty Rejected': 0, 'Qty for MRB': 0, 'Rework': nan, 'duration': '00:00.0'}]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepak/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecatedWarning: apply is deprecated as of 1.3.0 and will be removed in 2.0.0. Use algorithm entrypoint instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "log_csv = pd.read_csv('Production_Data.csv', sep=',')\n",
    "log = conversion_factory.apply(log_csv, parameters={constants.PARAMETER_CONSTANT_CASEID_KEY: \"Case ID\",\n",
    "                                               constants.PARAMETER_CONSTANT_ACTIVITY_KEY: \"Activity\",\n",
    "                                                constants.PARAMETER_CONSTANT_START_TIMESTAMP_KEY:\"Start Timestamp\",\n",
    "                                                constants.PARAMETER_CONSTANT_RESOURCE_KEY:\"Resource\",\n",
    "                                                constants.PARAMETER_CONSTANT_TIMESTAMP_KEY:\"Complete Timestamp\"\n",
    "                                               })\n",
    "print(\"Log imported\\n\\n\\n\")\n",
    "\n",
    "\n",
    "#print(log)\n",
    "'''for case_index, case in enumerate(log):\n",
    "    print(\"Caseeeeee:\",case)\n",
    "    #print(\"\\n case index: %d  case id: %s\" % (case_index, case.attributes[\"concept:name\"]))\n",
    "    for event_index, event in enumerate(case):\n",
    "        print(\"Eventtttttt:\",event)'''\n",
    "\n",
    "activities = attributes_filter.get_attribute_values(log, \"Case ID\")\n",
    "tracefilter_log_pos = attributes_filter.apply(log, [\"Case 1\"],\n",
    "                                          parameters={attributes_filter.Parameters.ATTRIBUTE_KEY : \"Case ID\", attributes_filter.Parameters.POSITIVE: True})\n",
    "print(\"###########################\\n\",tracefilter_log_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling main function\n",
    "#if __name__==\"__main__\": \n",
    "#    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
