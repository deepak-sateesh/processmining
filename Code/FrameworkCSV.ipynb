{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the imports here\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualization\n",
    "from pm4py.objects.conversion.dfg import converter as dfg_mining\n",
    "from pm4py.visualization.petrinet import factory as pn_vis_factory\n",
    "from collections import defaultdict \n",
    "import pandas as pd\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.adapters.pandas import csv_import_adapter\n",
    "from pm4py.objects.conversion.log import factory as conversion_factory\n",
    "from pm4py.util import constants\n",
    "import math\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from pm4py.objects.log.util import sorting\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_log(log):\n",
    "    for case_index, case in enumerate(log):\n",
    "        print(\"\\n case index: %d  case id: %s\" % (case_index, case.attributes[\"concept:name\"]))\n",
    "        for event_index, event in enumerate(case):\n",
    "            print(\"event index: %d  event activity: %s\" % (event_index, event[\"concept:name\"]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class represents a directed graph using dfg \n",
    "class Graph: \n",
    "\n",
    "    def __init__(self,vertices): \n",
    "        self.V= vertices #No. of vertices \n",
    "        self.graph = defaultdict(list) # default dictionary to store graph \n",
    "\n",
    "    # function to add an edge to graph \n",
    "    def addEdge(self,u,v): \n",
    "        self.graph[u].append(v) \n",
    "    \n",
    "    # Use BFS to check path between s and d \n",
    "    def isReachable(self, s, d): \n",
    "        # Mark all the vertices as not visited \n",
    "        visited =[False]*(self.V) \n",
    "\n",
    "        # Create a queue for BFS \n",
    "        queue=[]\n",
    "\n",
    "        # Mark the source node as visited and enqueue it \n",
    "        queue.append(s) \n",
    "        visited[s] = True\n",
    "\n",
    "        while queue: \n",
    "\n",
    "            #Dequeue a vertex from queue \n",
    "            n = queue.pop(0) \n",
    "\n",
    "            # If this adjacent node is the destination node, \n",
    "            # then return true \n",
    "            if n == d: \n",
    "                return True\n",
    "\n",
    "            # Else, continue to do BFS \n",
    "            for i in self.graph[n]: \n",
    "                if visited[i] == False: \n",
    "                    queue.append(i) \n",
    "                    visited[i] = True\n",
    "        # If BFS is complete without visited d \n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_weakness(log, forbidden_sequence):\n",
    "    #Weakness 1: Duplicate or loop-> Same event repeating twice in the log\n",
    "    for case_index, case in enumerate(log):\n",
    "        print(\"\\n case index: %d  case id: %s\" % (case_index, case.attributes[\"concept:name\"]))\n",
    "        event_list=[]\n",
    "        \n",
    "        for event_index, event in enumerate(case):\n",
    "            print(\"event index: %d  event activity: %s\" % (event_index, event[\"concept:name\"]))\n",
    "            event_list.append(event[\"concept:name\"])\n",
    "            \n",
    "        print (\"The events which got repeated in the trace are\",find_duplicate_events(event_list))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    #Weakness 2: Find out if the forbidden sequence of events exists in the log\n",
    "    #applying the Directly follows graph discovery to get the sequence which are directly following each other\n",
    "    dfg_simple = dfg_discovery.apply(log)\n",
    "    violated_restrictions=[]#for directly following each other or indirectly following\n",
    "    for r in forbidden_sequence:\n",
    "        count=0\n",
    "        for d in dfg_simple.elements():\n",
    "            if(r==d):\n",
    "                count+=1\n",
    "                violated_restrictions.append((r,count))\n",
    "            #else if(r[0]==d[0]):\n",
    "            \n",
    "              \n",
    "                \n",
    "    print(\"Violated restrictions, Number of times violated: \",violated_restrictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c754f8b0cee44c9ca0f3b61550f47f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='parsing log, completed traces :: ', max=6.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There is a path from register request to decide\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a graph for the given dfg\n",
    "\n",
    "log = xes_importer.apply('running-example.xes')\n",
    "dfg_simple = dfg_discovery.apply(log)\n",
    "\n",
    "    \n",
    "g = Graph(len(list(dfg_simple.elements())))\n",
    "l=[]\n",
    "for t in dfg_simple.elements(): \n",
    "    for x in t: \n",
    "        l.append(x) \n",
    "l=list(set(l))#list mapping every element to a number\n",
    "for d in dfg_simple.elements():\n",
    "    g.addEdge(l.index(d[0]),l.index(d[1]))\n",
    "    \n",
    "\n",
    "u =l.index(\"register request\"); v = l.index(\"decide\")\n",
    "\n",
    "if g.isReachable(u, v): \n",
    "    print(\"There is a path from %s to %s\" % (l[u],l[v])) \n",
    "else : \n",
    "    print(\"There is no path from %s to %s\" % (l[u],l[v])) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f954ae2ea5604e8cb0d34370893944dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='parsing log, completed traces :: ', max=6.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.Trace'>\n",
      "<class 'pm4py.objects.log.log.EventLog'>\n"
     ]
    }
   ],
   "source": [
    "log = xes_importer.apply('running-example.xes')\n",
    "dfg_simple = dfg_discovery.apply(log)\n",
    "\n",
    "for case_index, case in enumerate(log):\n",
    "    print(type(case))\n",
    "    #dfg_simple1 = dfg_discovery.apply(case)\n",
    "    \n",
    "\n",
    "print(type(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.objects.dfg.utils import dfg_utils\n",
    "from pm4py.objects.petri.petrinet import PetriNet, Marking\n",
    "from pm4py.objects.petri import utils as pn_util\n",
    "from enum import Enum\n",
    "from pm4py.util import exec_utils\n",
    "\n",
    "\n",
    "class Parameters(Enum):\n",
    "    START_ACTIVITIES = 'start_activities'\n",
    "    END_ACTIVITIES = 'end_activities'\n",
    "\n",
    "\n",
    "\n",
    "PARAM_KEY_START_ACTIVITIES = Parameters.START_ACTIVITIES\n",
    "PARAM_KEY_END_ACTIVITIES = Parameters.END_ACTIVITIES\n",
    "\n",
    "#obtain petrinet from dfg\n",
    "def obtain_petrinet_from_dfg(dfg, parameters=None):\n",
    "    \"\"\"\n",
    "    Applies the DFG mining on a given object (if it is a Pandas dataframe or a log, the DFG is calculated)\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    dfg\n",
    "        Object (DFG) (if it is a Pandas dataframe or a log, the DFG is calculated)\n",
    "    parameters\n",
    "        Parameters\n",
    "    \"\"\"\n",
    "    if parameters is None:\n",
    "        parameters = {}\n",
    "\n",
    "    dfg = dfg\n",
    "    start_activities = exec_utils.get_param_value(Parameters.START_ACTIVITIES, parameters,\n",
    "                                                  dfg_utils.infer_start_activities(\n",
    "                                                      dfg))\n",
    "    end_activities = exec_utils.get_param_value(Parameters.END_ACTIVITIES, parameters,\n",
    "                                                dfg_utils.infer_end_activities(dfg))\n",
    "    activities = dfg_utils.get_activities_from_dfg(dfg)\n",
    "\n",
    "    net = PetriNet(\"\")\n",
    "    im = Marking()\n",
    "    fm = Marking()\n",
    "\n",
    "    source = PetriNet.Place(\"source\")\n",
    "    net.places.add(source)\n",
    "    im[source] = 1\n",
    "    sink = PetriNet.Place(\"sink\")\n",
    "    net.places.add(sink)\n",
    "    fm[sink] = 1\n",
    "\n",
    "    places_corr = {}\n",
    "    index = 0\n",
    "\n",
    "    for act in activities:\n",
    "        places_corr[act] = PetriNet.Place(act)\n",
    "        net.places.add(places_corr[act])\n",
    "\n",
    "    for act in start_activities:\n",
    "        if act in places_corr:\n",
    "            index = index + 1\n",
    "            trans = PetriNet.Transition(act + \"_\" + str(index), act)\n",
    "            net.transitions.add(trans)\n",
    "            pn_util.add_arc_from_to(source, trans, net)\n",
    "            pn_util.add_arc_from_to(trans, places_corr[act], net)\n",
    "\n",
    "    for act in end_activities:\n",
    "        if act in places_corr:\n",
    "            index = index + 1\n",
    "            inv_trans = PetriNet.Transition(act + \"_\" + str(index), None)\n",
    "            net.transitions.add(inv_trans)\n",
    "            pn_util.add_arc_from_to(places_corr[act], inv_trans, net)\n",
    "            pn_util.add_arc_from_to(inv_trans, sink, net)\n",
    "\n",
    "    for el in dfg.keys():\n",
    "        act1 = el[0]\n",
    "        act2 = el[1]\n",
    "\n",
    "        index = index + 1\n",
    "        trans = PetriNet.Transition(act2 + \"_\" + str(index), act2)\n",
    "        net.transitions.add(trans)\n",
    "\n",
    "        pn_util.add_arc_from_to(places_corr[act1], trans, net)\n",
    "        pn_util.add_arc_from_to(trans, places_corr[act2], net)\n",
    "\n",
    "    return net, im, fm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'row={cols[0]:1, cols[1]: \\'Case 10\\', cols[2]:\\'FA\\', cols[3]:\\'1\\',cols[4]:\\'User\\',cols[5]:\\'10/02/2020 12:20\\',cols[6]:\\'Unwanted activity “a\"\\',cols[7]:\\'Happened in the case\\',cols[8]:\\'\\'}\\n#cols[3]:\\'1\\'\\ndf = df.append(row, ignore_index=True)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=['Detected Weakness Row','Case ID','Weakness Type (AF/PA)','Weakness ID','Weakness Origin', 'Weakness Time','Weakness Information','Weakness Measurement', 'Weakness Level']\n",
    "df=pd.DataFrame(columns=cols)\n",
    "#len(cols)\n",
    "\n",
    "'''row={cols[0]:1, cols[1]: 'Case 10', cols[2]:'FA', cols[3]:'1',cols[4]:'User',cols[5]:'10/02/2020 12:20',cols[6]:'Unwanted activity “a\"',cols[7]:'Happened in the case',cols[8]:''}\n",
    "#cols[3]:'1'\n",
    "df = df.append(row, ignore_index=True)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unwanted_Activity(log, blacklist):\n",
    "    global df, cols\n",
    "    print(\"Unwanted activity function\")\n",
    "    for case_index, case in enumerate(log):\n",
    "        for event_index, event in enumerate(case):\n",
    "            if(event[\"Activity\"] in blacklist):\n",
    "                #print(\"Unwanted activity=> activity: %s -> case: %s that started @ %s \" % (event[\"Activity\"], event[\"Case ID\"], event[\"Start Timestamp\"]))\n",
    "                row={cols[0]:event[\"Case ID\"]+\"-> Event \"+str(event_index), cols[1]: event[\"Case ID\"], cols[2]:'AF', cols[3]:'1',cols[4]:'Expert',cols[5]:event[\"Start Timestamp\"],cols[6]:'Unwanted activity \\\"'+event[\"Activity\"]+'\\\"',cols[7]:'In the case', cols[8]:'Event level'}\n",
    "                df=df.append(row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks for the largest common prefix  \n",
    "def lcp(s, t):  \n",
    "  n = min(len(s),len(t));  \n",
    "  for i in range(0,n):  \n",
    "    if(s[i] != t[i]):  \n",
    "      return s[0:i];  \n",
    "  else:  \n",
    "    return s[0:n];  \n",
    "\n",
    "def Find_sequence(eventList):\n",
    "    lrs=\"\";  \n",
    "    n = len(eventList);  \n",
    "    for i in range(0,n):  \n",
    "      for j in range(i+1,n):  \n",
    "        #Checks for the largest common factors in every substring  \n",
    "        x = lcp(eventList[i:n],eventList[j:n]);  \n",
    "            #If the current prefix is greater than previous one   \n",
    "            #then it takes the current one as longest repeating sequence  \n",
    "        if(len(x) > len(lrs)):\n",
    "            lrs=x;\n",
    "    \n",
    "          \n",
    "            \n",
    "    if(len(set(lrs))>1):\n",
    "        return (lrs);  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backloop(log):\n",
    "    print(\"Backloop function\")\n",
    "    global df\n",
    "    for case_index, case in enumerate(log):\n",
    "        eventList=[]\n",
    "        lrs=\"\"\n",
    "        indexList=[]\n",
    "        for event_index, event in enumerate(case):\n",
    "            eventList.append(event[\"Activity\"])  \n",
    "        if(Find_sequence(eventList) is not None ):\n",
    "            lrs=Find_sequence(eventList)\n",
    "            #print(\"Repeating sequence for events in case:\",case.attributes['concept:name'],\" is: \", lrs)  \n",
    "            row={cols[0]:case.attributes['concept:name'], cols[1]: case.attributes['concept:name'], cols[2]:'AF', cols[3]:'2',cols[4]:'Automatic detection',cols[5]:'',cols[6]:'Backloop {'+''.join(lrs)+'}',cols[7]:'In the case',cols[8]:'Case level'}\n",
    "            df=df.append(row, ignore_index=True)\n",
    "    #for trace in event_log:\n",
    "    #    print(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_events(x): \n",
    "    _size = len(x) \n",
    "    duplicate_list = [] \n",
    "    for i in range(_size): \n",
    "        k = i + 1\n",
    "        for j in range(k, _size): \n",
    "            if x[i] == x[j] and x[i] not in duplicate_list: \n",
    "                duplicate_list.append(x[i]) \n",
    "    return duplicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Redundant_Activity(log):\n",
    "    global df\n",
    "    print(\"Redundant_Activity function\")\n",
    "    for case_index, case in enumerate(log):\n",
    "        #print(\"\\n Case Id: %s\" % ( case.attributes[\"concept:name\"]))\n",
    "        event_list=[]\n",
    "        \n",
    "        for event_index, event in enumerate(case):\n",
    "            #print(\"event start time: %s  event activity: %s\" % (event[\"Start Timestamp\"], event[\"Activity\"]))\n",
    "            event_list.append(event[\"Activity\"])  \n",
    "        duplicateEventList=[]\n",
    "        duplicateEventList=find_duplicate_events(event_list)\n",
    "        #print (\"The events which got repeated in the trace are\",duplicateEventList)\n",
    "        if(len(duplicateEventList)>0):\n",
    "            row={cols[0]:case.attributes[\"concept:name\"], cols[1]: event[\"Case ID\"], cols[2]:'AF', cols[3]:'3',cols[4]:'Automatic detection',cols[5]:\"\",cols[6]:'Redundant Activities list: \\\"'+''.join(duplicateEventList)+'\\\"',cols[7]:'In the case', cols[8]:'Case Level'}\n",
    "            df=df.append(row, ignore_index=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interface(log):\n",
    "    print(\"Interface function\")\n",
    "    global df\n",
    "    for case_index, case in enumerate(log):\n",
    "        d={}\n",
    "        l=\"\"\n",
    "        #print(\"\\n Case Id: %s\" % ( case.attributes[\"concept:name\"]))\n",
    "        \n",
    "        '''for event_index, event in enumerate(case):\n",
    "            if( len(d)!=0 and event[\"Activity\"] in d.keys() and event[\"Resource\"]!= d[event[\"Activity\"]]):\n",
    "                print(\"The resource has changed for the activity: %s from %s to %s\"%(event[\"Activity\"], d[event[\"Activity\"]], event[\"Resource\"]))\n",
    "            d[event[\"Activity\"]]=event[\"Resource\"]'''\n",
    "        prev=\"\"\n",
    "        for event_index, event in enumerate(case):\n",
    "            if( prev!=\"\" and  event[\"Resource\"]!= prev):\n",
    "                #print(\"The resource has changed for the activity: \\\"%s\\\" from \\\"%s\\\" to \\\"%s\\\"\"%(event[\"Activity\"], prev, event[\"Resource\"]))\n",
    "                row={cols[0]:event[\"Case ID\"]+\"-> Event \"+str(event_index), cols[1]: event[\"Case ID\"], cols[2]:'AF', cols[3]:'4',cols[4]:'Automatic detection',cols[5]:event[\"Start Timestamp\"],cols[6]:'Change of interface for activity '+event[\"Activity\"]+' from ' +prev+' to '+ event[\"Resource\"],cols[7]:'In the case',cols[8]:'Event Level'}\n",
    "                df=df.append(row, ignore_index=True)\n",
    "\n",
    "\n",
    "            prev=event[\"Resource\"]\n",
    "            \n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Switch_of_media(log):\n",
    "    print(\"Switch_of_media function\")\n",
    "    print(\"Logic is same as Interface function as there is no column for media in the given CSV \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Idle_time(log,maxTime):\n",
    "    global df\n",
    "    print(\"Idle_time function\")\n",
    "    for case_index, case in enumerate(log):\n",
    "        #print(\"\\n Case Id: %s\" % ( case.attributes[\"concept:name\"]))\n",
    "        prev_end_timestamp=0\n",
    "        idle_time=0\n",
    "        prev_activity=\"\"\n",
    "        for event_index, event in enumerate(case):\n",
    "            if(prev_end_timestamp!=0):\n",
    "                idle_time=pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-prev_end_timestamp\n",
    "            #print(\"Idle time between previous activity:%s and current activity:%s is %s\"%(prev_activity, event[\"Activity\"], idle_time))\n",
    "            #if(type(idle_time)!= int):\n",
    "            #    print(idle_time.total_seconds  )#idle_time/np.timedelta64(1,'s'))\n",
    "            if(type(idle_time)!= int and idle_time.total_seconds()>maxTime):\n",
    "                #.total_seconds()>7200) :\n",
    "                row={cols[0]:event[\"Case ID\"]+\"-> Event \"+str(event_index), cols[1]: event[\"Case ID\"], cols[2]:'PA', cols[3]:'6',cols[4]:'Expert',cols[5]:event[\"Start Timestamp\"],cols[6]:'Idletime between '+prev_activity+' to ' +event[\"Activity\"]+' is '+ str(idle_time),cols[7]:'In the case',cols[8]:'Event level'}\n",
    "                df=df.append(row, ignore_index=True)\n",
    "            prev_end_timestamp=pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\") \n",
    "            prev_activity=event[\"Activity\"]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean1(log):\n",
    "    total_events=0\n",
    "    avg_dict={}\n",
    "    for case_index, case in enumerate(log): \n",
    "        for event_index, event in enumerate(case):\n",
    "            total_events=+1\n",
    "            if event[\"Activity\"] not in avg_dict.keys():\n",
    "                avg_dict[event[\"Activity\"]]=((pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\"))/ pd.Timedelta(hours=1),1)\n",
    "            else:\n",
    "                avg_dict[event[\"Activity\"]]=(avg_dict[event[\"Activity\"]][0] + (pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\"))/ pd.Timedelta(hours=1),(avg_dict[event[\"Activity\"]][1])+1)\n",
    "    \n",
    "    avg_dict2={}\n",
    "    for k, v in avg_dict.items():\n",
    "        avg_dict2[k]=v[0]/v[1]\n",
    "    print(avg_dict2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Variance_of_process_times(log):\n",
    "    global df\n",
    "    print(\"Variance_of_process_times function\")\n",
    "    d={}#mean1(log)\n",
    "    l=[]\n",
    "    for case_index, case in enumerate(log): \n",
    "        for event_index, event in enumerate(case):\n",
    "            if event[\"Activity\"] not in d.keys():\n",
    "                l=[]\n",
    "            else:\n",
    "                l=d[event[\"Activity\"]]\n",
    "            l.append( (pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\"))/ pd.Timedelta(hours=1))\n",
    "            d[event[\"Activity\"]]=(l)\n",
    "    variance_dict={}\n",
    "    for k,v in d.items():\n",
    "        variance_dict[k]= (min(v), max(v),np.mean(v) ,np.var(v))\n",
    "        row={cols[0]:\"All Activities\", cols[1]: k, cols[2]:'PA', cols[3]:'7',cols[4]:'Automatic detection',cols[5]:'',cols[6]:'(Min, Max, Average, Variance) for current activity:'+str((min(v), max(v),np.mean(v) ,np.var(v))),cols[7]:'In the Activity',cols[8]:'Activity Level'}\n",
    "        df=df.append(row, ignore_index=True)\n",
    "    #print(\"(Min, Max, Average, Variance) for each activity:\")\n",
    "    #print(variance_dict)\n",
    "    \n",
    "    \n",
    "   \n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bottleneck(log):\n",
    "    global df\n",
    "    print(\"Bottleneck function\")\n",
    "    '''for case_index, case in enumerate(log):\n",
    "        print(\"\\n Case Id: %s\" % ( case.attributes[\"concept:name\"]))\n",
    "        duration=0 \n",
    "        a=\"\"\n",
    "        max_duration=0\n",
    "        for event_index, event in enumerate(case):\n",
    "            duration=pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")\n",
    "            if(max_duration==0 or duration>max_duration):\n",
    "                max_duration=duration\n",
    "                a=event[\"Activity\"]\n",
    "        print(\"Bottleneck Activity at case level:%s took maximum time of %s to complete\"%(a,max_duration ))'''\n",
    "    duration=0 \n",
    "    a=\"\"\n",
    "    max_duration=0\n",
    "    for case_index, case in enumerate(log):\n",
    "        for event_index, event in enumerate(case):\n",
    "            duration=pd.to_datetime(event[\"Complete Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")-pd.to_datetime(event[\"Start Timestamp\"], format = \"%m/%d/%Y %H:%M:%S\")\n",
    "            if(max_duration==0 or duration>max_duration):\n",
    "                max_duration=duration\n",
    "                a=event[\"Activity\"]\n",
    "    \n",
    "    print(\"Bottleneck Activity on log level:%s took maximum time of %s to complete\"%(a,max_duration ))\n",
    "    row={cols[0]:\"All Activities\", cols[1]: a, cols[2]:'PA', cols[3]:'8',cols[4]:'Automatic detection',cols[5]:'',cols[6]:'Activity took maximum time of '+str(max_duration),cols[7]:'In the Activity',cols[8]:'Log Level'}\n",
    "    df=df.append(row, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallelizable_tasks_loglevel():\n",
    "    print(\"Parallelizable_tasks function\\n\\n\")\n",
    "    log_csv2 = pd.read_csv('Production_Data.csv', sep=',')\n",
    "    log_csv2.rename(columns={'Activity': 'concept:name'}, inplace=True)\n",
    "    parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'Case ID'}\n",
    "    event_log2 = log_converter.apply(log_csv2, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "    #print(event_log)\n",
    "    dfg_simple2 = dfg_discovery.apply(event_log2)\n",
    "    #print(dfg_simple2)\n",
    "    #('Turning & Milling Q.C.', 'Turning & Milling - Machine 8'): 27,\n",
    "    for k in dfg_simple2.keys():\n",
    "        if(k[0]!=k[1]):\n",
    "            if (k[1],k[0]) in dfg_simple2.keys():\n",
    "                print(k,\" : are Parallelizable activities\")\n",
    "         \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parallelizable_tasks_CaseLevel():\n",
    "    global df\n",
    "    print(\"Parallelizable_tasks_CaseLevel function\\n\\n\")\n",
    "    log_csv3 = pd.read_csv('Production_Data.csv', sep=',')\n",
    "    log_csv3.rename(columns={'Activity': 'concept:name'}, inplace=True)\n",
    "    parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'Case ID'}\n",
    "    event_log3 = log_converter.apply(log_csv3, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "    event_log3 = sorting.sort_timestamp(event_log3,\"Start Timestamp\", False)\n",
    "    \n",
    "    for case_index, case in enumerate(event_log3):\n",
    "        \n",
    "        tracefilter_log_pos = attributes_filter.apply(event_log3, [case.attributes[\"concept:name\"]],\n",
    "                                          parameters={attributes_filter.Parameters.ATTRIBUTE_KEY : \"Case ID\", attributes_filter.Parameters.POSITIVE: True})\n",
    "        \n",
    "        dfg_simple3 = dfg_discovery.apply(tracefilter_log_pos)\n",
    "        l=[]\n",
    "        for k in dfg_simple3.keys():\n",
    "            if(k[0]!=k[1]):\n",
    "                if (k[1],k[0]) in dfg_simple3.keys():\n",
    "                    l.append((k[1],k[0]))\n",
    "        l1=[]\n",
    "        for i in l:\n",
    "            if (i[1],i[0]) in l:\n",
    "                l1.append((i[0],i[1]))\n",
    "                l.remove((i[1],i[0]))\n",
    "                l.remove((i[0],i[1]))\n",
    "        if(len(l)>0)   :  \n",
    "            row={cols[0]:case.attributes['concept:name'], cols[1]: case.attributes['concept:name'], cols[2]:'AF', cols[3]:'9',cols[4]:'Automatic detection',cols[5]:'',cols[6]:'Parallelizable tasks :'+''.join(str(l1)),cols[7]:'In the case',cols[8]:'Case level'}\n",
    "            df=df.append(row, ignore_index=True)\n",
    "            #print(\"\\n\\nParallelizable tasks for Case:\",case.attributes[\"concept:name\"],\" are => \", end=\" \")\n",
    "            #print(l1)\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Joint Master thesis:\n",
      "Modelling of production expertise to extend the data-driven analysis of process models\n",
      "Log imported\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepak/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: DeprecatedWarning: apply is deprecated as of 1.3.0 and will be removed in 2.0.0. Use algorithm entrypoint instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unwanted activity function\n",
      "Backloop function\n",
      "Redundant_Activity function\n",
      "Interface function\n",
      "Idle_time function\n",
      "Variance_of_process_times function\n",
      "Bottleneck function\n",
      "Bottleneck Activity on log level:Turning & Milling - Machine 9 took maximum time of 0 days 23:50:00 to complete\n",
      "Parallelizable_tasks_CaseLevel function\n",
      "\n",
      "\n",
      "     Detected Weakness Row   Case ID Weakness Type (AF/PA) Weakness ID  \\\n",
      "0       Case 203-> Event 0  Case 203                    AF           1   \n",
      "1       Case 203-> Event 2  Case 203                    AF           1   \n",
      "2      Case 207-> Event 20  Case 207                    AF           1   \n",
      "3      Case 207-> Event 22  Case 207                    AF           1   \n",
      "4      Case 207-> Event 24  Case 207                    AF           1   \n",
      "...                    ...       ...                   ...         ...   \n",
      "4039               Case 42   Case 42                    AF           9   \n",
      "4040              Case 127  Case 127                    AF           9   \n",
      "4041              Case 122  Case 122                    AF           9   \n",
      "4042              Case 112  Case 112                    AF           9   \n",
      "4043              Case 174  Case 174                    AF           9   \n",
      "\n",
      "          Weakness Origin       Weakness Time  \\\n",
      "0                  Expert   1/10/2012 0:00:00   \n",
      "1                  Expert   1/11/2012 8:09:00   \n",
      "2                  Expert   1/23/2012 0:00:00   \n",
      "3                  Expert  1/23/2012 14:57:00   \n",
      "4                  Expert  1/23/2012 16:54:00   \n",
      "...                   ...                 ...   \n",
      "4039  Automatic detection                       \n",
      "4040  Automatic detection                       \n",
      "4041  Automatic detection                       \n",
      "4042  Automatic detection                       \n",
      "4043  Automatic detection                       \n",
      "\n",
      "                                   Weakness Information Weakness Measurement  \\\n",
      "0               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
      "1               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
      "2               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
      "3               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
      "4               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
      "...                                                 ...                  ...   \n",
      "4039  Parallelizable tasks :[('Turning & Milling Q.C...          In the case   \n",
      "4040  Parallelizable tasks :[('Lapping - Machine 1',...          In the case   \n",
      "4041  Parallelizable tasks :[('Round Grinding - Mach...          In the case   \n",
      "4042  Parallelizable tasks :[('Turning & Milling Q.C...          In the case   \n",
      "4043  Parallelizable tasks :[('Turning & Milling Q.C...          In the case   \n",
      "\n",
      "     Weakness Level  \n",
      "0       Event level  \n",
      "1       Event level  \n",
      "2       Event level  \n",
      "3       Event level  \n",
      "4       Event level  \n",
      "...             ...  \n",
      "4039     Case level  \n",
      "4040     Case level  \n",
      "4041     Case level  \n",
      "4042     Case level  \n",
      "4043     Case level  \n",
      "\n",
      "[4044 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Defining main function \n",
    "def main(): \n",
    "    print(\"Welcome to Joint Master thesis:\\nModelling of production expertise to extend the data-driven analysis of process models\") \n",
    "    \n",
    "    '''#Import a log\n",
    "    log = xes_importer.apply('running-example.xes')\n",
    "    print(\"Log imported\")\n",
    "    \n",
    "    #Explore the log\n",
    "    #explore_log(log)\n",
    "    \n",
    "    #Define the forbidden sequence of events\n",
    "    #simple restriction which says you cannot decide without examining thoroughly \n",
    "    forbidden_sequence=[( 'decide','examine thoroughly')]\n",
    "    \n",
    "    #Find different kinds of weakness in the log\n",
    "    find_weakness(log, forbidden_sequence)\n",
    "    \n",
    "    #obtain_petrinet_from_dfg\n",
    "    dfg_simple = dfg_discovery.apply(log)\n",
    "    net, im, fm = obtain_petrinet_from_dfg(dfg_simple)\n",
    "\n",
    "    #Visualise the petrinet obtained\n",
    "    gviz = pn_vis_factory.apply(net, im, fm)\n",
    "    pn_vis_factory.view(gviz)'''\n",
    "    \n",
    "    #log = xes_importer.apply('running-example.xes')\n",
    "    log_csv = pd.read_csv('Production_Data.csv', sep=',')\n",
    "    log = conversion_factory.apply(log_csv, parameters={constants.PARAMETER_CONSTANT_CASEID_KEY: \"Case ID\",\n",
    "                                                   constants.PARAMETER_CONSTANT_ACTIVITY_KEY: \"Activity\",\n",
    "                                                    constants.PARAMETER_CONSTANT_START_TIMESTAMP_KEY:\"Start Timestamp\",\n",
    "                                                    constants.PARAMETER_CONSTANT_RESOURCE_KEY:\"Resource\",\n",
    "                                                    constants.PARAMETER_CONSTANT_TIMESTAMP_KEY:\"Complete Timestamp\"\n",
    "                                                   })\n",
    "    print(\"Log imported\\n\\n\\n\")\n",
    "    \n",
    "    logSorted = sorting.sort_timestamp(log,\"Start Timestamp\", False)\n",
    "    #cols=['Detected Weakness Row','Case ID','Weakness Type (AF/PA)','Weakness ID','Weakness Origin', 'Weakness Time','Weakness Information','Weakness Measurement']\n",
    "    #df=pd.DataFrame(columns=cols)\n",
    "    #print(log)'sorted_log' x.attributes[\"concept:name\"], x.events[\"Start Timestamp\"]\n",
    "    #for i in log:\n",
    "    #    print(sorted(i),events['Start Timestamp'])\n",
    "    \n",
    "    blacklist=[ 'Lapping - Machine 1','Turning & Milling - Machine 8']\n",
    "    Unwanted_Activity(logSorted, blacklist)\n",
    "    \n",
    "    Backloop(logSorted)\n",
    "    \n",
    "    Redundant_Activity(logSorted) \n",
    "    \n",
    "    Interface(logSorted)\n",
    "    \n",
    "    #Switch_of_media(logSorted) Same as Interface as there is no column for media in the given CSV \n",
    "    maxTime=86400\n",
    "    Idle_time(logSorted, maxTime)\n",
    "    \n",
    "    Variance_of_process_times(logSorted)\n",
    "    \n",
    "    Bottleneck(logSorted)\n",
    "    \n",
    "    ##Parallelizable_tasks_loglevel()\n",
    "    Parallelizable_tasks_CaseLevel()\n",
    "    print(df)\n",
    "    \n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log imported\n",
      "\n",
      "\n",
      "\n",
      "[{'attributes': {'concept:name': 'Case 1'}, 'events': [{'Case ID': 'Case 1', 'Activity': 'Turning & Milling - Machine 4', 'Resource': 'Machine 4 - Turning & Milling', 'Start Timestamp': '1/29/2012 23:24:00', 'Complete Timestamp': '1/30/2012 5:43:00', 'Span': '006:19', 'Work Order  Qty': 10, 'Part Desc.': 'Cable Head', 'Worker ID': 'ID4932', 'Report Type': 'S', 'Qty Completed': 1, 'Qty Rejected': 0, 'Qty for MRB': 0, 'Rework': nan, 'duration': '19:00.0'}, '..', {'Case ID': 'Case 1', 'Activity': 'Packing', 'Resource': 'Packing', 'Start Timestamp': '2/17/2012 0:00:00', 'Complete Timestamp': '2/17/2012 1:00:00', 'Span': '000:00', 'Work Order  Qty': 10, 'Part Desc.': 'Cable Head', 'Worker ID': 'ID4820', 'Report Type': 'D', 'Qty Completed': 9, 'Qty Rejected': 0, 'Qty for MRB': 0, 'Rework': nan, 'duration': '00:00.0'}]}, '....', {'attributes': {'concept:name': 'Case 99'}, 'events': [{'Case ID': 'Case 99', 'Activity': 'Turning & Milling Q.C.', 'Resource': 'Quality Check 1', 'Start Timestamp': '3/22/2012 11:59:00', 'Complete Timestamp': '3/22/2012 16:03:00', 'Span': '004:04', 'Work Order  Qty': 507, 'Part Desc.': 'Drill', 'Worker ID': 'ID4618', 'Report Type': 'D', 'Qty Completed': 0, 'Qty Rejected': 0, 'Qty for MRB': 0, 'Rework': nan, 'duration': '04:00.0'}, '..', {'Case ID': 'Case 99', 'Activity': 'Packing', 'Resource': 'Packing', 'Start Timestamp': '3/30/2012 0:00:00', 'Complete Timestamp': '3/30/2012 1:00:00', 'Span': '000:00', 'Work Order  Qty': 507, 'Part Desc.': 'Drill', 'Worker ID': 'ID4820', 'Report Type': 'D', 'Qty Completed': 160, 'Qty Rejected': 0, 'Qty for MRB': 0, 'Rework': nan, 'duration': '00:00.0'}]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepak/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecatedWarning: apply is deprecated as of 1.3.0 and will be removed in 2.0.0. Use algorithm entrypoint instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "log_csv = pd.read_csv('Production_Data.csv', sep=',')\n",
    "log = conversion_factory.apply(log_csv, parameters={constants.PARAMETER_CONSTANT_CASEID_KEY: \"Case ID\",\n",
    "                                               constants.PARAMETER_CONSTANT_ACTIVITY_KEY: \"Activity\",\n",
    "                                                constants.PARAMETER_CONSTANT_START_TIMESTAMP_KEY:\"Start Timestamp\",\n",
    "                                                constants.PARAMETER_CONSTANT_RESOURCE_KEY:\"Resource\",\n",
    "                                                constants.PARAMETER_CONSTANT_TIMESTAMP_KEY:\"Complete Timestamp\"\n",
    "                                               })\n",
    "print(\"Log imported\\n\\n\\n\")\n",
    "\n",
    "\n",
    "print(log)\n",
    "'''for case_index, case in enumerate(log):\n",
    "    print(\"Caseeeeee:\",case)\n",
    "    #print(\"\\n case index: %d  case id: %s\" % (case_index, case.attributes[\"concept:name\"]))\n",
    "    for event_index, event in enumerate(case):\n",
    "        print(\"Eventtttttt:\",event)'''\n",
    "\n",
    "activities = attributes_filter.get_attribute_values(log, \"Case ID\")\n",
    "tracefilter_log_pos = attributes_filter.apply(log, [\"Case 1\"],\n",
    "                                          parameters={attributes_filter.Parameters.ATTRIBUTE_KEY : \"Case ID\", attributes_filter.Parameters.POSITIVE: True})\n",
    "#print(\"###########################\\n\",tracefilter_log_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'attributes': {'concept:name': 'Case 178'}, 'events': [{'Case ID': 'Case 178', 'concept:name': 'Round Grinding - Machine 3', 'Resource': 'Machine 3 - Round Grinding', 'time:timestamp': Timestamp('2012-01-02 00:00:00'), 'Complete Timestamp': '1/2/2012 4:50:00', 'Span': '004:50', 'Work Order  Qty': 250, 'Part Desc.': 'Cable Head', 'Worker ID': 'ID4445', 'Report Type': 'D', 'Qty Completed': 31, 'Qty Rejected': 0, 'Qty for MRB': 0, 'Rework': nan, 'duration': '50:00.0'}, '..', {'Case ID': 'Case 178', 'concept:name': 'Round Grinding - Q.C.', 'Resource': 'Quality Check 1', 'time:timestamp': Timestamp('2012-03-14 00:00:00'), 'Complete Timestamp': '3/14/2012 1:15:00', 'Span': '000:00', 'Work Order  Qty': 250, 'Part Desc.': 'Cable Head', 'Worker ID': 'ID0937', 'Report Type': 'D', 'Qty Completed': 245, 'Qty Rejected': 0, 'Qty for MRB': 0, 'Rework': nan, 'duration': '15:00.0'}]}, '....', {'attributes': {'concept:name': 'Case 107'}, 'events': [{'Case ID': 'Case 107', 'concept:name': 'Turning & Milling - Machine 10', 'Resource': 'Machine 10 - Grinding', 'time:timestamp': Timestamp('2012-03-30 06:36:00'), 'Complete Timestamp': '3/30/2012 11:47:00', 'Span': '005:11', 'Work Order  Qty': 20, 'Part Desc.': 'Cable Head', 'Worker ID': 'ID4132', 'Report Type': 'S', 'Qty Completed': 1, 'Qty Rejected': 0, 'Qty for MRB': 0, 'Rework': nan, 'duration': '11:00.0'}]}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_csv4 = pd.read_csv('Production_Data.csv', sep=',')\n",
    "log_csv4.rename(columns={'Activity': 'concept:name','Start Timestamp':'time:timestamp'}, inplace=True)\n",
    "log_csv4['time:timestamp'] =  pd.to_datetime(log_csv4['time:timestamp'] , format='%m/%d/%Y %H:%M:%S')\n",
    "parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'Case ID'}\n",
    "event_log4 = log_converter.apply(log_csv4, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "event_log4 = sorting.sort_timestamp(event_log4,\"time:timestamp\", False)\n",
    "event_log4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling main function\n",
    "#if __name__==\"__main__\": \n",
    "#    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detected Weakness Row</th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Weakness Type (AF/PA)</th>\n",
       "      <th>Weakness ID</th>\n",
       "      <th>Weakness Origin</th>\n",
       "      <th>Weakness Time</th>\n",
       "      <th>Weakness Information</th>\n",
       "      <th>Weakness Measurement</th>\n",
       "      <th>Weakness Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case 203-&gt; Event 0</td>\n",
       "      <td>Case 203</td>\n",
       "      <td>AF</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>1/10/2012 0:00:00</td>\n",
       "      <td>Unwanted activity \"Lapping - Machine 1\"</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Event level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case 203-&gt; Event 2</td>\n",
       "      <td>Case 203</td>\n",
       "      <td>AF</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>1/11/2012 8:09:00</td>\n",
       "      <td>Unwanted activity \"Lapping - Machine 1\"</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Event level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case 207-&gt; Event 20</td>\n",
       "      <td>Case 207</td>\n",
       "      <td>AF</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>1/23/2012 0:00:00</td>\n",
       "      <td>Unwanted activity \"Lapping - Machine 1\"</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Event level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case 207-&gt; Event 22</td>\n",
       "      <td>Case 207</td>\n",
       "      <td>AF</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>1/23/2012 14:57:00</td>\n",
       "      <td>Unwanted activity \"Lapping - Machine 1\"</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Event level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case 207-&gt; Event 24</td>\n",
       "      <td>Case 207</td>\n",
       "      <td>AF</td>\n",
       "      <td>1</td>\n",
       "      <td>Expert</td>\n",
       "      <td>1/23/2012 16:54:00</td>\n",
       "      <td>Unwanted activity \"Lapping - Machine 1\"</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Event level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>Case 42</td>\n",
       "      <td>Case 42</td>\n",
       "      <td>AF</td>\n",
       "      <td>9</td>\n",
       "      <td>Automatic detection</td>\n",
       "      <td></td>\n",
       "      <td>Parallelizable tasks :[('Turning &amp; Milling Q.C...</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Case level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>Case 127</td>\n",
       "      <td>Case 127</td>\n",
       "      <td>AF</td>\n",
       "      <td>9</td>\n",
       "      <td>Automatic detection</td>\n",
       "      <td></td>\n",
       "      <td>Parallelizable tasks :[('Lapping - Machine 1',...</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Case level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>Case 122</td>\n",
       "      <td>Case 122</td>\n",
       "      <td>AF</td>\n",
       "      <td>9</td>\n",
       "      <td>Automatic detection</td>\n",
       "      <td></td>\n",
       "      <td>Parallelizable tasks :[('Round Grinding - Mach...</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Case level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>Case 112</td>\n",
       "      <td>Case 112</td>\n",
       "      <td>AF</td>\n",
       "      <td>9</td>\n",
       "      <td>Automatic detection</td>\n",
       "      <td></td>\n",
       "      <td>Parallelizable tasks :[('Turning &amp; Milling Q.C...</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Case level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>Case 174</td>\n",
       "      <td>Case 174</td>\n",
       "      <td>AF</td>\n",
       "      <td>9</td>\n",
       "      <td>Automatic detection</td>\n",
       "      <td></td>\n",
       "      <td>Parallelizable tasks :[('Turning &amp; Milling Q.C...</td>\n",
       "      <td>In the case</td>\n",
       "      <td>Case level</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4044 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Detected Weakness Row   Case ID Weakness Type (AF/PA) Weakness ID  \\\n",
       "0       Case 203-> Event 0  Case 203                    AF           1   \n",
       "1       Case 203-> Event 2  Case 203                    AF           1   \n",
       "2      Case 207-> Event 20  Case 207                    AF           1   \n",
       "3      Case 207-> Event 22  Case 207                    AF           1   \n",
       "4      Case 207-> Event 24  Case 207                    AF           1   \n",
       "...                    ...       ...                   ...         ...   \n",
       "4039               Case 42   Case 42                    AF           9   \n",
       "4040              Case 127  Case 127                    AF           9   \n",
       "4041              Case 122  Case 122                    AF           9   \n",
       "4042              Case 112  Case 112                    AF           9   \n",
       "4043              Case 174  Case 174                    AF           9   \n",
       "\n",
       "          Weakness Origin       Weakness Time  \\\n",
       "0                  Expert   1/10/2012 0:00:00   \n",
       "1                  Expert   1/11/2012 8:09:00   \n",
       "2                  Expert   1/23/2012 0:00:00   \n",
       "3                  Expert  1/23/2012 14:57:00   \n",
       "4                  Expert  1/23/2012 16:54:00   \n",
       "...                   ...                 ...   \n",
       "4039  Automatic detection                       \n",
       "4040  Automatic detection                       \n",
       "4041  Automatic detection                       \n",
       "4042  Automatic detection                       \n",
       "4043  Automatic detection                       \n",
       "\n",
       "                                   Weakness Information Weakness Measurement  \\\n",
       "0               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
       "1               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
       "2               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
       "3               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
       "4               Unwanted activity \"Lapping - Machine 1\"          In the case   \n",
       "...                                                 ...                  ...   \n",
       "4039  Parallelizable tasks :[('Turning & Milling Q.C...          In the case   \n",
       "4040  Parallelizable tasks :[('Lapping - Machine 1',...          In the case   \n",
       "4041  Parallelizable tasks :[('Round Grinding - Mach...          In the case   \n",
       "4042  Parallelizable tasks :[('Turning & Milling Q.C...          In the case   \n",
       "4043  Parallelizable tasks :[('Turning & Milling Q.C...          In the case   \n",
       "\n",
       "     Weakness Level  \n",
       "0       Event level  \n",
       "1       Event level  \n",
       "2       Event level  \n",
       "3       Event level  \n",
       "4       Event level  \n",
       "...             ...  \n",
       "4039     Case level  \n",
       "4040     Case level  \n",
       "4041     Case level  \n",
       "4042     Case level  \n",
       "4043     Case level  \n",
       "\n",
       "[4044 rows x 9 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unwanted activity \"Lapping - Machine 1\"'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Weakness Information'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"AF_dataframe.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pm4py.statistics.traces.log import case_statistics\n",
    "all_case_durations = case_statistics.get_all_casedurations(event_log4, parameters={\n",
    "    case_statistics.Parameters.TIMESTAMP_KEY: \"time:timestamp\"})\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_case_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197480.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_case_duration = case_statistics.get_median_caseduration(event_log4, parameters={\n",
    "    case_statistics.Parameters.TIMESTAMP_KEY: \"time:timestamp\"\n",
    "})\n",
    "median_case_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20070.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pm4py.statistics.traces.log import case_arrival\n",
    "case_arrival_ratio = case_arrival.get_case_arrival_avg(event_log4, parameters={\n",
    "    case_arrival.Parameters.TIMESTAMP_KEY: \"time:timestamp\"})\n",
    "case_arrival_ratio      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'list_activities': ['Final Inspection Q.C.', 'Packing'],\n",
       " 'points': [[1325671200.0, 1325721600.0],\n",
       "  [1325683200.0, 1325721600.0],\n",
       "  [1325690760.0, 1325808000.0],\n",
       "  [1325750700.0, 1325808000.0],\n",
       "  [1325757180.0, 1325808000.0],\n",
       "  [1326006000.0, 1326240000.0],\n",
       "  [1326031440.0, 1326067200.0],\n",
       "  [1326183660.0, 1326240000.0],\n",
       "  [1326207600.0, 1326326400.0],\n",
       "  [1326277320.0, 1326672000.0],\n",
       "  [1326283440.0, 1326412800.0],\n",
       "  [1326288240.0, 1326585600.0],\n",
       "  [1326356400.0, 1326585600.0],\n",
       "  [1326445200.0, 1326585600.0],\n",
       "  [1326447540.0, 1326585600.0],\n",
       "  [1326456180.0, 1326585600.0],\n",
       "  [1326457500.0, 1326585600.0],\n",
       "  [1326610200.0, 1326672000.0],\n",
       "  [1326611940.0, 1326758400.0],\n",
       "  [1326619200.0, 1326672000.0],\n",
       "  [1326629640.0, 1326758400.0],\n",
       "  [1326630600.0, 1326844800.0],\n",
       "  [1326805200.0, 1326844800.0],\n",
       "  [1326873600.0, 1329091200.0],\n",
       "  [1326882000.0, 1326931200.0],\n",
       "  [1326956400.0, 1327190400.0],\n",
       "  [1326967200.0, 1327017600.0],\n",
       "  [1327215300.0, 1327276800.0],\n",
       "  [1327217160.0, 1327276800.0],\n",
       "  [1327243200.0, 1327276800.0],\n",
       "  [1327363200.0, 1327363200.0],\n",
       "  [1327398480.0, 1327449600.0],\n",
       "  [1327398660.0, 1327449600.0],\n",
       "  [1327419000.0, 1330560000.0],\n",
       "  [1327577400.0, 1327622400.0],\n",
       "  [1327649880.0, 1327795200.0],\n",
       "  [1327662720.0, 1327795200.0],\n",
       "  [1327665600.0, 1327881600.0],\n",
       "  [1327820400.0, 1327881600.0],\n",
       "  [1327821660.0, 1327881600.0],\n",
       "  [1327827600.0, 1327881600.0],\n",
       "  [1327843200.0, 1327881600.0],\n",
       "  [1327844520.0, 1327881600.0],\n",
       "  [1327931640.0, 1327968000.0],\n",
       "  [1328165340.0, 1328227200.0],\n",
       "  [1328167800.0, 1328400000.0],\n",
       "  [1328175000.0, 1328486400.0],\n",
       "  [1328175360.0, 1328400000.0],\n",
       "  [1328186280.0, 1328227200.0],\n",
       "  [1328189280.0, 1328227200.0],\n",
       "  [1328196600.0, 1328400000.0],\n",
       "  [1328252400.0, 1328486400.0],\n",
       "  [1328448000.0, 1328486400.0],\n",
       "  [1328452740.0, 1328486400.0],\n",
       "  [1328512440.0, 1328572800.0],\n",
       "  [1328597220.0, 1329091200.0],\n",
       "  [1328692440.0, 1329350400.0],\n",
       "  [1328705040.0, 1328745600.0],\n",
       "  [1328788800.0, 1328832000.0],\n",
       "  [1328872200.0, 1329004800.0],\n",
       "  [1329030000.0, 1329091200.0],\n",
       "  [1329115980.0, 1331164800.0],\n",
       "  [1329127200.0, 1329177600.0],\n",
       "  [1329151200.0, 1329177600.0],\n",
       "  [1329210000.0, 1329264000.0],\n",
       "  [1329230700.0, 1329264000.0],\n",
       "  [1329312000.0, 1329350400.0],\n",
       "  [1329315600.0, 1329350400.0],\n",
       "  [1329390900.0, 1329436800.0],\n",
       "  [1329396180.0, 1329436800.0],\n",
       "  [1329398400.0, 1329436800.0],\n",
       "  [1329399000.0, 1329436800.0],\n",
       "  [1329401520.0, 1329436800.0],\n",
       "  [1329406320.0, 1329436800.0],\n",
       "  [1329471000.0, 1330214400.0],\n",
       "  [1329634200.0, 1329696000.0],\n",
       "  [1329634800.0, 1329696000.0],\n",
       "  [1329657600.0, 1329696000.0],\n",
       "  [1329657600.0, 1329782400.0],\n",
       "  [1329660420.0, 1329696000.0],\n",
       "  [1329664500.0, 1329782400.0],\n",
       "  [1329721200.0, 1329782400.0],\n",
       "  [1329721320.0, 1329782400.0],\n",
       "  [1329726360.0, 1329868800.0],\n",
       "  [1329728820.0, 1329782400.0],\n",
       "  [1329736260.0, 1329782400.0],\n",
       "  [1329752040.0, 1329782400.0],\n",
       "  [1329753180.0, 1329782400.0],\n",
       "  [1329807600.0, 1329955200.0],\n",
       "  [1329812040.0, 1329868800.0],\n",
       "  [1329822420.0, 1329868800.0],\n",
       "  [1329826500.0, 1329955200.0],\n",
       "  [1329841080.0, 1329868800.0],\n",
       "  [1329896760.0, 1329955200.0],\n",
       "  [1329901200.0, 1329955200.0],\n",
       "  [1329927000.0, 1329955200.0],\n",
       "  [1329980400.0, 1330041600.0],\n",
       "  [1329982800.0, 1330473600.0],\n",
       "  [1329992460.0, 1330041600.0],\n",
       "  [1330007400.0, 1330473600.0],\n",
       "  [1330214400.0, 1330214400.0],\n",
       "  [1330267620.0, 1330300800.0],\n",
       "  [1330333200.0, 1330387200.0],\n",
       "  [1330336800.0, 1330387200.0],\n",
       "  [1330348800.0, 1330387200.0],\n",
       "  [1330352040.0, 1330387200.0],\n",
       "  [1330352160.0, 1330387200.0],\n",
       "  [1330357860.0, 1330387200.0],\n",
       "  [1330358100.0, 1330387200.0],\n",
       "  [1330412940.0, 1330473600.0],\n",
       "  [1330413660.0, 1330473600.0],\n",
       "  [1330419600.0, 1330905600.0],\n",
       "  [1330435200.0, 1330473600.0],\n",
       "  [1330443420.0, 1330473600.0],\n",
       "  [1330445160.0, 1330473600.0],\n",
       "  [1330446480.0, 1330473600.0],\n",
       "  [1330447500.0, 1330473600.0],\n",
       "  [1330502880.0, 1332201600.0],\n",
       "  [1330504200.0, 1332028800.0],\n",
       "  [1330608000.0, 1330646400.0],\n",
       "  [1330853100.0, 1331424000.0],\n",
       "  [1330861080.0, 1330905600.0],\n",
       "  [1330867200.0, 1330905600.0],\n",
       "  [1331043060.0, 1331078400.0],\n",
       "  [1331051040.0, 1331078400.0],\n",
       "  [1331130600.0, 1331164800.0],\n",
       "  [1331135100.0, 1331164800.0],\n",
       "  [1331189400.0, 1331251200.0],\n",
       "  [1331204400.0, 1331251200.0],\n",
       "  [1331215260.0, 1331251200.0],\n",
       "  [1331453880.0, 1331510400.0],\n",
       "  [1331472000.0, 1331510400.0],\n",
       "  [1331551200.0, 1331769600.0],\n",
       "  [1331553540.0, 1331553600.0],\n",
       "  [1331554620.0, 1331596800.0],\n",
       "  [1331555220.0, 1331596800.0],\n",
       "  [1331625600.0, 1331683200.0],\n",
       "  [1331630460.0, 1331683200.0],\n",
       "  [1331657040.0, 1331683200.0],\n",
       "  [1331707980.0, 1332892800.0],\n",
       "  [1331723580.0, 1331769600.0],\n",
       "  [1331726580.0, 1331769600.0],\n",
       "  [1331794680.0, 1331856000.0],\n",
       "  [1331810340.0, 1331856000.0],\n",
       "  [1331821800.0, 1332028800.0],\n",
       "  [1331892840.0, 1332028800.0],\n",
       "  [1332061200.0, 1332115200.0],\n",
       "  [1332065340.0, 1332201600.0],\n",
       "  [1332140400.0, 1332201600.0],\n",
       "  [1332144000.0, 1332288000.0],\n",
       "  [1332154680.0, 1332201600.0],\n",
       "  [1332156060.0, 1332288000.0],\n",
       "  [1332163200.0, 1332374400.0],\n",
       "  [1332164340.0, 1332374400.0],\n",
       "  [1332172800.0, 1332201600.0],\n",
       "  [1332237600.0, 1332288000.0],\n",
       "  [1332238500.0, 1332288000.0],\n",
       "  [1332259200.0, 1332288000.0],\n",
       "  [1332263040.0, 1332288000.0],\n",
       "  [1332264000.0, 1332374400.0],\n",
       "  [1332324960.0, 1332374400.0],\n",
       "  [1332336000.0, 1332374400.0],\n",
       "  [1332343620.0, 1332374400.0],\n",
       "  [1332487500.0, 1332633600.0],\n",
       "  [1332498600.0, 1332633600.0],\n",
       "  [1332658200.0, 1332720000.0],\n",
       "  [1332666000.0, 1332720000.0],\n",
       "  [1332668940.0, 1332979200.0],\n",
       "  [1332669600.0, 1332720000.0],\n",
       "  [1332775800.0, 1332806400.0],\n",
       "  [1332838800.0, 1332892800.0],\n",
       "  [1332844200.0, 1332892800.0],\n",
       "  [1332854760.0, 1332892800.0],\n",
       "  [1332925800.0, 1332979200.0],\n",
       "  [1332927600.0, 1332979200.0],\n",
       "  [1332930060.0, 1332979200.0],\n",
       "  [1332938280.0, 1333065600.0],\n",
       "  [1332943980.0, 1333065600.0],\n",
       "  [1333008000.0, 1333065600.0],\n",
       "  [1333012800.0, 1333065600.0],\n",
       "  [1333022400.0, 1333065600.0],\n",
       "  [1333023840.0, 1333065600.0],\n",
       "  [1333036620.0, 1333065600.0],\n",
       "  [1333040400.0, 1333065600.0],\n",
       "  [1333065600.0, 1333065600.0]]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pm4py.statistics.performance_spectrum import algorithm as performance_spectrum\n",
    "ps = performance_spectrum.apply(event_log4, [\"Final Inspection Q.C.\",\"Packing\"], parameters={performance_spectrum.Parameters.ACTIVITY_KEY: \"concept:name\",\n",
    "                                            performance_spectrum.Parameters.TIMESTAMP_KEY: \"time:timestamp\"})\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lifecycle:transition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9e7906da1a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpm4py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterval_lifecycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menriched_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterval_lifecycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_lead_cycle_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_log4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menriched_log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pm4py/objects/log/util/interval_lifecycle.py\u001b[0m in \u001b[0;36massign_lead_cycle_time\u001b[0;34m(log, parameters)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mweekends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weekends\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"weekends\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0minterval_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minterval_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pm4py/objects/log/util/interval_lifecycle.py\u001b[0m in \u001b[0;36mto_interval\u001b[0;34m(log, parameters)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mactivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactivity_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransition_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimestamp_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lifecycle:transition'"
     ]
    }
   ],
   "source": [
    "from pm4py.objects.log.util import interval_lifecycle\n",
    "enriched_log = interval_lifecycle.assign_lead_cycle_time(event_log4)\n",
    "enriched_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.statistics.sojourn_time.log import get as soj_time_get\n",
    "\n",
    "soj_time = soj_time_get.apply(event_log4, parameters={soj_time_get.Parameters.TIMESTAMP_KEY: \"time:timestamp\", soj_time_get.Parameters.START_TIMESTAMP_KEY: \"start_timestamp\"})\n",
    "print(soj_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
